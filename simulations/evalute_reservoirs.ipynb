{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from common import load_output_data_hydroshoot, load_vertices, load_output_data_wheatfspm, load_output_data_grassleaf, generate_groups_and_masks, nmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the Hydroshoot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fn_data_hydroshoot = \"Hydroshoot/gdc_can1_grapevine.csv\"\n",
    "vtc = load_vertices(fn_data_hydroshoot)\n",
    "vtc_selected = random.sample(vtc, 32)\n",
    "dfo_hydroshoot = load_output_data_hydroshoot(fn_data_hydroshoot, vtc_selected, \"E\")\n",
    "dfi_hydroshoot = pd.read_csv(\"Hydroshoot/meteo.csv\", sep=\";\")\n",
    "dfi_hydroshoot.loc[:,\"time\"] = pd.to_datetime(dfi_hydroshoot.loc[:,\"time\"])\n",
    "dfip_hydroshoot = load_output_data_hydroshoot(fn_data_hydroshoot, None, \"Ei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find start of the data trace\n",
    "with open(fn_data_hydroshoot) as f:\n",
    "    header = f.readline().split(\",\")\n",
    "    time_idx = header.index(\"t\")\n",
    "    data = f.readline().split(\",\")\n",
    "    t0 = pd.to_datetime(data[time_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mask = dfi_hydroshoot[\"time\"] >= t0.to_datetime64()\n",
    "dfi_hydroshoot = dfi_hydroshoot[mask].reset_index(drop=True)\n",
    "dfi_hydroshoot = dfi_hydroshoot.drop(index = range(len(dfo_hydroshoot), len(dfi_hydroshoot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in dfo_hydroshoot:\n",
    "    plt.plot(dfo_hydroshoot[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfo_hydroshoot.reset_index(drop=True, inplace=True)\n",
    "dfo_hydroshoot.drop(index=range(15*24,int(dfo_hydroshoot.index[-1]+1)),inplace=True)\n",
    "dfo_hydroshoot.reset_index(drop=True, inplace=True)\n",
    "dfi_hydroshoot.reset_index(drop=True, inplace=True)\n",
    "dfi_hydroshoot.drop(index=range(15*24,int(dfi_hydroshoot.index[-1]+1)),inplace=True)\n",
    "dfi_hydroshoot.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfip_hydroshoot.reset_index(drop=True, inplace=True)\n",
    "dfip_hydroshoot.drop(index=range(15*24,int(dfip_hydroshoot.index[-1]+1)),inplace=True)\n",
    "dfip_hydroshoot.reset_index(drop=True, inplace=True)\n",
    "dfip_hydroshoot = dfip_hydroshoot.to_numpy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{len(dfo_hydroshoot)} {len(dfi_hydroshoot)} {len(dfip_hydroshoot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load the WheatFspm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"WheatFSPM/NEMA15/elements_postprocessing.csv\")\n",
    "df2 = pd.read_csv(\"WheatFSPM/NEMA15/elements_states.csv\")\n",
    "for k in df2.keys():\n",
    "    if k not in df1:\n",
    "        df1[k] = df2[k]\n",
    "df1.to_csv(\"WheatFspm/NEMA15_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fn_data_wheatfspm = \"WheatFSPM/NEMA15_dataset.csv\"\n",
    "dfo_wheatfspm = load_output_data_wheatfspm(fn_data_wheatfspm, \"Transpiration\")\n",
    "dfi_wheatfspm = pd.read_csv(\"WheatFSPM/meteo.csv\", index_col=\"t\")\n",
    "dfip_wheatfspm = load_output_data_wheatfspm(fn_data_wheatfspm, \"PARa\")\n",
    "dfi_wheatfspm = dfi_wheatfspm.drop(index = range(len(dfo_wheatfspm), len(dfi_wheatfspm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in dfo_wheatfspm:\n",
    "    plt.plot(dfo_wheatfspm[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfo_wheatfspm.reset_index(drop=True,inplace=True)\n",
    "dfo_wheatfspm.drop(index=range(18*24,int(dfo_wheatfspm.index[-1]+1)),inplace=True)\n",
    "dfo_wheatfspm.reset_index(drop=True,inplace=True)\n",
    "dfi_wheatfspm.reset_index(drop=True,inplace=True)\n",
    "dfi_wheatfspm.drop(index=range(18*24,int(dfi_wheatfspm.index[-1]+1)),inplace=True)\n",
    "dfi_wheatfspm.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfip_wheatfspm.reset_index(drop=True,inplace=True)\n",
    "dfip_wheatfspm.drop(index=range(18*24,int(dfip_wheatfspm.index[-1]+1)),inplace=True)\n",
    "dfip_wheatfspm.reset_index(drop=True,inplace=True)\n",
    "dfip_wheatfspm = dfip_wheatfspm.to_numpy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grass Leaf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fn_data_grassleaf = \"GrassLeaf/model_output_length.csv\"\n",
    "dfo_grassleaf = load_output_data_grassleaf(fn_data_grassleaf)\n",
    "dfi_grassleaf = pd.read_csv(\"GrassLeaf/meteo.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in dfo_grassleaf:\n",
    "    plt.plot(dfo_grassleaf[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfo_grassleaf.drop(index=range(48,11*24),inplace=True)\n",
    "dfo_grassleaf.reset_index(drop=True, inplace=True)\n",
    "dfi_grassleaf.drop(index=range(48,11*24),inplace=True)\n",
    "dfi_grassleaf.reset_index(drop=True, inplace=True)\n",
    "dfi_grassleaf.drop(index=range(len(dfo_grassleaf), len(dfi_grassleaf)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{len(dfo_grassleaf)} {len(dfi_grassleaf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, we have to make sure that all datasets are of the same length to make the comparison fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sim_len = min([len(dfo_grassleaf), len(dfo_wheatfspm), len(dfo_hydroshoot), 18*24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sim_len//24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#if sim_len < len(dfo_hydroshoot):\n",
    "#    dfo_hydroshoot = dfo_hydroshoot.drop(index=range(sim_len, len(dfo_hydroshoot)))\n",
    "#    dfi_hydroshoot = dfi_hydroshoot.drop(index=range(sim_len, len(dfi_hydroshoot)))\n",
    "#    dfip_hydroshoot = dfip_hydroshoot[:sim_len]\n",
    "#if sim_len < len(dfo_wheatfspm):\n",
    "#    dfo_wheatfspm = dfo_wheatfspm.drop(index=range(sim_len, len(dfo_wheatfspm)))\n",
    "#    dfi_wheatfspm = dfi_wheatfspm.drop(index=range(sim_len, len(dfi_wheatfspm)))\n",
    "#    dfip_wheatfspm = dfip_wheatfspm[:sim_len]\n",
    "#if sim_len < len(dfo_grassleaf):\n",
    "#    dfo_grassleaf = dfo_grassleaf.drop(index=range(sim_len, len(dfo_grassleaf)))\n",
    "#    dfo_grassleaf = dfo_grassleaf.drop(index=range(sim_len, len(dfo_grassleaf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dfi_grassleaf = dfi_grassleaf.drop(index = range(len(dfo_grassleaf), len(dfi_grassleaf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "Generate model and CV hyperparameter tuning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimise\n",
    "\n",
    "Run the model, optimise and report the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for dfo, dfi, dfip, label, y_labels in zip(\n",
    "        [dfo_hydroshoot, dfo_wheatfspm, dfo_grassleaf],\n",
    "        [dfi_hydroshoot, dfi_wheatfspm, dfi_grassleaf],\n",
    "        [dfip_hydroshoot, dfip_wheatfspm, None],\n",
    "        [\"Hydroshoot\", \"WheatFspm\", \"GrassLeaf\"],\n",
    "        [(\"Rg\", \"Tac\"), (\"PARi\", \"air_temperature\"), (\"Rg\", \"Tac\")]):\n",
    "    print(f\"Running {label} dataset\")\n",
    "\n",
    "    groups, train_mask, test_mask = generate_groups_and_masks(dfo, discard=(22,2), n_day_split=3)\n",
    "\n",
    "    X = dfo.to_numpy()\n",
    "    X_train = X[train_mask, :]\n",
    "    X_test = X[test_mask, :]\n",
    "    y1 = dfi[y_labels[0]].to_numpy()\n",
    "    y2 = dfi[y_labels[1]].to_numpy()\n",
    "    yp = dfip\n",
    "    y1_train = y1[train_mask]\n",
    "    y1_test = y1[test_mask]\n",
    "    y2_train = y2[train_mask]\n",
    "    y2_test = y2[test_mask]\n",
    "    yp_train = yp[train_mask] if dfip is not None else None\n",
    "    yp_test = yp[test_mask] if dfip is not None else None\n",
    "    groups_train = groups[train_mask]\n",
    "    groups_test = groups[test_mask]\n",
    "\n",
    "    for y_train, y_test, target in zip([y1_train, y2_train, yp_train], [y1_test, y2_test, yp_test], [\"PAR\", \"Tair\", \"PARa\"]):\n",
    "\n",
    "        if y_test is None:\n",
    "            continue\n",
    "\n",
    "        search_grid = {\"model__alpha\": np.logspace(-6,6,100)}\n",
    "        scorer = make_scorer(nmse, greater_is_better=False)\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"model\", Ridge(fit_intercept=True, solver=\"svd\"))])\n",
    "        cv = LeaveOneGroupOut()\n",
    "        grid = GridSearchCV(estimator=pipe, param_grid=search_grid, cv=cv, scoring=scorer, return_train_score=True, refit=True)\n",
    "\n",
    "        grid.fit(X_train, y_train, groups=groups_train)\n",
    "        results_df = pd.DataFrame(grid.cv_results_)\n",
    "        results_df['median_test_score'] = results_df.filter(regex='^split').median(axis=1)\n",
    "        results_df['rank_test_score'] = results_df['median_test_score'].rank(ascending=False).astype(int)\n",
    "\n",
    "        pipe.set_params(**results_df.query('rank_test_score == 1')['params'].values[0])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        print(f\"Optimal hyperparameter value for {target}: {grid.best_params_}\")\n",
    "\n",
    "        t = np.arange(0, len(dfo)) / 24.0\n",
    "        t = t[test_mask]\n",
    "        t2 = np.arange(len(t)) / 24.0\n",
    "        yh_test = pipe.predict(X_test)\n",
    "        print(f\"Performance: {nmse(y_test, yh_test)} (vs) {nmse(y_train, pipe.predict(X_train))}\")\n",
    "        #plt.plot(t2, y_test, label=\"dataset\")\n",
    "        #plt.plot(t2, yh_test, label=\"prediction\")\n",
    "        plt.plot(y_test, label=\"dataset\")\n",
    "        #print(grid.best_estimator_.named_steps['model'].coef_)\n",
    "        #print(grid.best_estimator_.named_steps['model'].intercept_)\n",
    "        plt.plot(yh_test, label=\"prediction\")\n",
    "        plt.title(target)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"time\": t,\n",
    "            \"time2\": t2,\n",
    "            \"y\": y_test,\n",
    "            \"yh\": yh_test,\n",
    "        })\n",
    "\n",
    "        df.to_csv(f\"results_{label}-{target}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for dfo, dfi, dfip, label, y_labels in zip(\n",
    "        [dfo_hydroshoot, dfo_wheatfspm, dfo_grassleaf],\n",
    "        [dfi_hydroshoot, dfi_wheatfspm, dfi_grassleaf],\n",
    "        [dfip_hydroshoot, dfip_wheatfspm, None],\n",
    "        [\"Hydroshoot\", \"WheatFspm\", \"GrassLeaf\"],\n",
    "        [(\"Rg\", \"Tac\"), (\"PARi\", \"air_temperature\"), (\"Rg\", \"Tac\")]):\n",
    "    print(f\"Running {label} dataset\")\n",
    "\n",
    "    groups, train_mask, test_mask = generate_groups_and_masks(dfo, discard=(22,2), n_day_split=3)\n",
    "\n",
    "    X = dfo.to_numpy()\n",
    "\n",
    "    y1 = dfi[y_labels[0]].to_numpy()\n",
    "    y2 = dfi[y_labels[1]].to_numpy()\n",
    "    yp = dfip\n",
    "    y1_train = y1[train_mask]\n",
    "    y1_test = y1[test_mask]\n",
    "    y2_train = y2[train_mask]\n",
    "    y2_test = y2[test_mask]\n",
    "    yp_train = yp[train_mask] if dfip is not None else None\n",
    "    yp_test = yp[test_mask] if dfip is not None else None\n",
    "    groups_train = groups[train_mask]\n",
    "    groups_test = groups[test_mask]\n",
    "\n",
    "    for y_train, y_test, target in zip([y1_train], [y1_test], [\"PAR\"]):\n",
    "\n",
    "        performance_list_y_test = []\n",
    "        performance_list_y_train = []\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "\n",
    "            X_train = X[train_mask, :]\n",
    "            X_train = np.delete(X_train, i, axis=1)\n",
    "            X_test = X[test_mask, :]\n",
    "            X_test = np.delete(X_test, i, axis=1)\n",
    "\n",
    "            search_grid = {\"model__alpha\": np.logspace(-6,6,100)}\n",
    "            scorer = make_scorer(nmse, greater_is_better=False)\n",
    "            pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"model\", Ridge(fit_intercept=True, solver=\"svd\"))])\n",
    "            cv = LeaveOneGroupOut()\n",
    "            grid = GridSearchCV(estimator=pipe, param_grid=search_grid, cv=cv, scoring=scorer, return_train_score=True, refit=True)\n",
    "\n",
    "            grid.fit(X_train, y_train, groups=groups_train)\n",
    "            results_df = pd.DataFrame(grid.cv_results_)\n",
    "            results_df['median_test_score'] = results_df.filter(regex='^split').median(axis=1)\n",
    "            results_df['rank_test_score'] = results_df['median_test_score'].rank(ascending=False).astype(int)\n",
    "\n",
    "            pipe.set_params(**results_df.query('rank_test_score == 1')['params'].values[0])\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            print(f\"Optimal hyperparameter value for {target}: {grid.best_params_}\")\n",
    "\n",
    "            t = np.arange(0, len(dfo)) / 24.0\n",
    "            t = t[test_mask]\n",
    "            t2 = np.arange(len(t)) / 24.0\n",
    "            yh_test = pipe.predict(X_test)\n",
    "\n",
    "            performance_list_y_test.append(nmse(y_test, yh_test))\n",
    "            performance_list_y_train.append(nmse(y_train, pipe.predict(X_train)))\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"nmse_train\": performance_list_y_train,\n",
    "            \"nmse_test\": performance_list_y_test\n",
    "        })\n",
    "\n",
    "        df.to_csv(f\"error_{label}-{target}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}